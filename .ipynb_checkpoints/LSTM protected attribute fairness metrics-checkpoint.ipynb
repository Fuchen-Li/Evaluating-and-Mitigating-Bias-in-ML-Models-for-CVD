{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "741e6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from os import path\n",
    "\n",
    "import src.lib.utility_classfier_tuning as uclf\n",
    "import src.lib.optimal_threhold_related as thres\n",
    "import src.lib.fairness_tests as fair\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n",
    "import sklearn.preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c8d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path='/Users/lifuchen/Desktop/Evaluating-and-Mitigating-Bias-in-ML-Models-for-CVD/deep learning/results/'\n",
    "filename = \"LSTM gender prediction_0.csv\"\n",
    "prediction = pd.read_csv(path.join(result_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9b093e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_score = prediction['test_score'][prediction['test_score'].notna()]   \n",
    "y_test_score_male = prediction['test_male_score'][prediction['test_male_score'].notna()]   \n",
    "y_test_score_female = prediction['test_female_score'][prediction['test_female_score'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d4b3fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.190115\n",
       "1       0.140922\n",
       "2       0.172150\n",
       "3       0.182603\n",
       "4       0.015570\n",
       "          ...   \n",
       "7702    0.163235\n",
       "7703    0.077149\n",
       "7704    0.032254\n",
       "7705    0.020503\n",
       "7706    0.039099\n",
       "Name: test_male_score, Length: 7707, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_score_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "267c4991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result (records, auc_vec, ba_vec, eod_vec, di_vec, y_test, y_score, y_test_1, y_score_1, y_test_2, y_score_2):    \n",
    "    threshold = 0.1\n",
    "    ba = thres.calculate_balanced_accuracy(y_test, y_score, threshold)\n",
    "    auroc = roc_auc_score(y_test, y_score)\n",
    "    precision, recall, tpr, tnr, pd_overall = thres.calculate_precision_metrics(y_test, y_score,threshold)\n",
    "    \n",
    "    ba_male = thres.calculate_balanced_accuracy(y_test_1, y_score_1, threshold)\n",
    "    precision_male, recall_male, tpr_male, tnr_male, pd_male = thres.calculate_precision_metrics(y_test_1, y_score_1,threshold)\n",
    "    \n",
    "    ba_female = thres.calculate_balanced_accuracy (y_test_2, y_score_2, threshold)\n",
    "    precision_female, recall_female, tpr_female, tnr_female, pd_female = thres.calculate_precision_metrics(y_test_2, y_score_2,threshold)\n",
    "\n",
    "    eod = fair.get_EOD(y_test_1, y_score_1,threshold, y_test_2, y_score_2, threshold)\n",
    "    di = fair.get_SP(y_test_1, y_score_1,threshold, y_test_2, y_score_2, threshold)\n",
    "    \n",
    "    auc_vec.append(auroc)\n",
    "    ba_vec.append(ba)\n",
    "    eod_vec.append(eod)\n",
    "    di_vec.append(di)\n",
    "\n",
    "    records.append({\n",
    "        'auroc': auroc,\n",
    "        'overall ba test': ba,\n",
    "        'male ba test': ba_male,\n",
    "        'female ba test': ba_female,\n",
    "        'overall precision':precision,\n",
    "        'overall recall':recall,\n",
    "        'overall tpr':tpr,\n",
    "        'overall tnr':tnr,\n",
    "        'overall pd':pd_overall,\n",
    "        'male precision':precision_male,\n",
    "        'male recall':recall_male,\n",
    "        'male tpr':tpr_male,\n",
    "        'male tnr':tnr_male,\n",
    "        'male pd':pd_male,\n",
    "        'female precision':precision_female,\n",
    "        'female recall':recall_female,\n",
    "        'female tpr':tpr_female,\n",
    "        'female tnr':tnr_female,\n",
    "        'female pd':pd_female,\n",
    "        'eod': eod,\n",
    "        'di': di,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "804a7cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate of class 1 is  0.711\n",
      "True positive rate of class 2 is  0.57\n",
      "Positive prediction rate of class 1 is  0.388\n",
      "Positive prediction rate of class 2 is  0.292\n",
      "True positive rate of class 1 is  0.732\n",
      "True positive rate of class 2 is  0.556\n",
      "Positive prediction rate of class 1 is  0.383\n",
      "Positive prediction rate of class 2 is  0.27\n",
      "True positive rate of class 1 is  0.71\n",
      "True positive rate of class 2 is  0.581\n",
      "Positive prediction rate of class 1 is  0.372\n",
      "Positive prediction rate of class 2 is  0.29\n",
      "True positive rate of class 1 is  0.769\n",
      "True positive rate of class 2 is  0.736\n",
      "Positive prediction rate of class 1 is  0.421\n",
      "Positive prediction rate of class 2 is  0.344\n",
      "True positive rate of class 1 is  0.744\n",
      "True positive rate of class 2 is  0.67\n",
      "Positive prediction rate of class 1 is  0.409\n",
      "Positive prediction rate of class 2 is  0.322\n"
     ]
    }
   ],
   "source": [
    "records_lstm_race = []\n",
    "auc_race = []\n",
    "ba_race = []\n",
    "eod_race = []\n",
    "di_race = []\n",
    "for i in range(5):\n",
    "    filename = \"LSTM race prediction_\" + str(i) + \".csv\"\n",
    "    prediction = pd.read_csv(path.join(result_path, filename))\n",
    "    \n",
    "    y_test_score = prediction['test_score'][prediction['test_score'].notna()]   \n",
    "    y_test_score_white = prediction['test_white_score'][prediction['test_white_score'].notna()]   \n",
    "    y_test_score_black = prediction['test_black_score'][prediction['test_black_score'].notna()]\n",
    "    \n",
    "    filename_2 = \"LSTM result_\" + str(i) + \".csv\"\n",
    "    result = pd.read_csv(path.join(result_path, filename_2))\n",
    "    y_test = result['y_test'][result['y_test'].notna()] \n",
    "    y_test_white = result['y_test_white'][result['y_test_white'].notna()] \n",
    "    y_test_black = result['y_test_black'][result['y_test_black'].notna()] \n",
    "    \n",
    "    get_result(records_lstm_race, auc_race, ba_race, eod_race, di_race, y_test, y_test_score, y_test_white, y_test_score_white, y_test_black, y_test_score_black)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a366b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7317515545699996, 0.745816040171713, 0.7461133861376896, 0.759811045995653, 0.7519486982232731] 0.7470881450196657 0.009200832559424619\n",
      "[0.6741392726343165, 0.6878810240879287, 0.6831939051890283, 0.6950820333062526, 0.6851265459879972] 0.6850845562411048 0.0067991960974672205\n",
      "[0.14100000000000001, 0.17599999999999993, 0.129, 0.03300000000000003, 0.07399999999999995] 0.11059999999999999 0.05079606283955478\n",
      "[1.3287671232876714, 1.4185185185185185, 1.2827586206896553, 1.2238372093023255, 1.2701863354037266] 1.3048135614403795 0.06594728984486171\n"
     ]
    }
   ],
   "source": [
    "print(auc_race, np.mean(auc_race), np.std(auc_race))\n",
    "print(ba_race, np.mean(ba_race), np.std(ba_race))\n",
    "print(eod_race, np.mean(eod_race), np.std(eod_race))\n",
    "print(di_race, np.mean(di_race), np.std(di_race))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b20fa64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive rate of class 1 is  0.798\n",
      "True positive rate of class 2 is  0.612\n",
      "Positive prediction rate of class 1 is  0.523\n",
      "Positive prediction rate of class 2 is  0.295\n",
      "True positive rate of class 1 is  0.75\n",
      "True positive rate of class 2 is  0.68\n",
      "Positive prediction rate of class 1 is  0.445\n",
      "Positive prediction rate of class 2 is  0.325\n",
      "True positive rate of class 1 is  0.775\n",
      "True positive rate of class 2 is  0.63\n",
      "Positive prediction rate of class 1 is  0.474\n",
      "Positive prediction rate of class 2 is  0.299\n",
      "True positive rate of class 1 is  0.835\n",
      "True positive rate of class 2 is  0.711\n",
      "Positive prediction rate of class 1 is  0.513\n",
      "Positive prediction rate of class 2 is  0.354\n",
      "True positive rate of class 1 is  0.786\n",
      "True positive rate of class 2 is  0.698\n",
      "Positive prediction rate of class 1 is  0.474\n",
      "Positive prediction rate of class 2 is  0.355\n"
     ]
    }
   ],
   "source": [
    "records_lstm_gender = []\n",
    "auc_gender = []\n",
    "ba_gender = []\n",
    "eod_gender = []\n",
    "di_gender = []\n",
    "for i in range(5):\n",
    "    filename = \"LSTM gender prediction_\" + str(i) + \".csv\"\n",
    "    prediction = pd.read_csv(path.join(result_path, filename))\n",
    "    \n",
    "    y_test_score = prediction['test_score'][prediction['test_score'].notna()]   \n",
    "    y_test_score_male = prediction['test_male_score'][prediction['test_male_score'].notna()]   \n",
    "    y_test_score_female = prediction['test_female_score'][prediction['test_female_score'].notna()]\n",
    "    \n",
    "    filename_2 = \"LSTM result_\" + str(i) + \".csv\"\n",
    "    result = pd.read_csv(path.join(result_path, filename_2))\n",
    "    y_test = result['y_test'][result['y_test'].notna()] \n",
    "    y_test_male = result['y_test_male'][result['y_test_male'].notna()] \n",
    "    y_test_female = result['y_test_female'][result['y_test_female'].notna()] \n",
    "    \n",
    "    get_result(records_lstm_gender, auc_gender, ba_gender, eod_gender, di_gender, y_test, y_test_score, y_test_male, y_test_score_male, y_test_female, y_test_score_female)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "498fbb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7317515545699996, 0.745816040171713, 0.7461133861376896, 0.759811045995653, 0.7519486982232731] 0.7470881450196657 0.009200832559424619\n",
      "[0.6741392726343165, 0.6878810240879287, 0.6831939051890283, 0.6950820333062526, 0.6851265459879972] 0.6850845562411048 0.0067991960974672205\n",
      "[0.18600000000000005, 0.06999999999999995, 0.14500000000000002, 0.124, 0.08800000000000008] 0.12260000000000001 0.041200000000000014\n",
      "[1.7728813559322036, 1.3692307692307693, 1.585284280936455, 1.4491525423728815, 1.3352112676056338] 1.5023520432155886 0.16034423247567478\n"
     ]
    }
   ],
   "source": [
    "print(auc_gender, np.mean(auc_gender), np.std(auc_gender))\n",
    "print(ba_gender, np.mean(ba_gender), np.std(ba_gender))\n",
    "print(eod_gender, np.mean(eod_gender), np.std(eod_gender))\n",
    "print(di_gender, np.mean(di_gender), np.std(di_gender))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (CVDPrediction-master)",
   "language": "python",
   "name": "pycharm-6defbc71"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
